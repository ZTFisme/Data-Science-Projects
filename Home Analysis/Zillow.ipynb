{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import requests\n",
    "import lxml\n",
    "from lxml.html.soupparser import fromstring\n",
    "import prettify\n",
    "import numbers\n",
    "import htmltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "with requests.Session() as s:\n",
    "    #city = 'dallas'\n",
    "    \n",
    "    base_url = 'https://www.zillow.com/homes/for_sale/'\n",
    "    search_url = '?searchQueryState=%7B\"pagination\"%3A%7B%7D%2C\"mapBounds\"%3A%7B\"west\"%3A-97.32331467871094%2C\"east\"%3A-96.2439079404297%2C\"south\"%3A32.782893111556746%2C\"north\"%3A33.457573709558645%7D%2C\"customRegionId\"%3A\"ed70e61b24X1-CRlk9c6vnwe2ke_xlf96\"%2C\"isMapVisible\"%3Afalse%2C\"filterState\"%3A%7B\"ah\"%3A%7B\"value\"%3Atrue%7D%2C\"sort\"%3A%7B\"value\"%3A\"globalrelevanceex\"%7D%2C\"price\"%3A%7B\"max\"%3A500000%7D%2C\"mp\"%3A%7B\"max\"%3A1666%7D%7D%2C\"isListVisible\"%3Atrue%7D'\n",
    "    url = base_url + search_url\n",
    "    r = s.get(url, headers=req_headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    \n",
    "    import math\n",
    "    t = str(soup.title)\n",
    "    home_count = int(re.split(r'Homes For Sale - ', t)[1].split(' ')[0])\n",
    "    pages = math.ceil(home_count/40)    \n",
    "\n",
    "    for i in range(2,pages):\n",
    "        print(i)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    url2 = base_url+'2_p/' + search_url\n",
    "    url3 = base_url+'3_p/' + search_url\n",
    "    url4 = base_url+'4_p/' + search_url\n",
    "    url5 = base_url+'5_p/' + search_url\n",
    "    url6 = base_url+'6_p/' + search_url\n",
    "    url7 = base_url+'7_p/' + search_url\n",
    "    url8 = base_url+'8_p/' + search_url\n",
    "    url9 = base_url+'9_p/' + search_url\n",
    "    url10 = base_url+'10_p/' + search_url\n",
    "\n",
    "\n",
    "\n",
    "    r = s.get(url, headers=req_headers)\n",
    "    r2 = s.get(url2, headers=req_headers)\n",
    "    r3 = s.get(url3, headers=req_headers)\n",
    "    r4 = s.get(url4, headers=req_headers)\n",
    "    r5 = s.get(url5, headers=req_headers)\n",
    "    r6 = s.get(url6, headers=req_headers)\n",
    "    r7 = s.get(url7, headers=req_headers)\n",
    "    r8 = s.get(url8, headers=req_headers)\n",
    "    r9 = s.get(url9, headers=req_headers)\n",
    "    r10 = s.get(url10, headers=req_headers)\n",
    "    \n",
    "    url_links = [url, url2, url3, url4, url5, url6, url7, url8, url9, url10]\n",
    "    \n",
    "    #print(url_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrape(soup):\n",
    "    #all for loops are pulling the specified variable using beautiful soup and inserting into said variable\n",
    "    df2 = pd.DataFrame()\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    for i in range(len(address)):\n",
    "        address[i] = str(address[i]).split('<address class=\"list-card-addr\">')[1].split('</address>')[0]\n",
    "    price = list(soup.find_all (class_='list-card-price')) ## results, just need to re-format\n",
    "    for i in range(len(price)):\n",
    "        price[i] = int(str(price[i]).split('<div class=\"list-card-price\">$')[1].split('</div>')[0].replace(',',''))\n",
    "    details = list(soup.find_all(\"ul\", class_=\"list-card-details\")) ## long results, can parse beds, sqft, home_type\n",
    "    beds = []\n",
    "    sqft = []\n",
    "    home_type = []\n",
    "\n",
    "    for i in range(len(details)):\n",
    "        z = str(details[i])\n",
    "        beds.append(int(z.split('<ul class=\"list-card-details\"><li class=\"\">')[1].split('<')[0]))\n",
    "        sqft.append(int(z.split('<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li class=\"\">')[1].split('<')[0].replace(',', '')))\n",
    "        home_type.append(z.split('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li><li class=\"list-card-statusText\">- ')[1].split(' ')[0])\n",
    "    urls = []\n",
    "    for link in soup.find_all('a', {'class': 'list-card-link'}, href=True):\n",
    "        urls.append(link['href'])\n",
    "    urls = list(set(urls))\n",
    "    \n",
    "    zillow_zestimate = []\n",
    "    for i in range(len(urls)):\n",
    "        link = urls[i]\n",
    "        r = s.get(link, headers=req_headers)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        home_value = str(soup.find_all('span', {'class':'Text-c11n-8-53-2__sc-aiai24-0 fBcVfR'})[0])\n",
    "        if not home_value:\n",
    "            home_value = soup.select_one('.zestimate').text.split()[-1]\n",
    "        else:\n",
    "            home_value = int(home_value.split('<span class=\"Text-c11n-8-53-2__sc-aiai24-0 fBcVfR\">$')[1].split('</span>')[0].replace(',',''))\n",
    "        zillow_zestimate.append(home_value)\n",
    "    \n",
    "    df['Address'] = address\n",
    "    df['Beds'] = beds\n",
    "    df['SQFT'] = sqft\n",
    "    df['Home_Type'] = home_type\n",
    "    df['Price'] = price\n",
    "    df['Link'] = urls\n",
    "    df['Zestimate'] = zillow_zestimate\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>Beds</th>\n",
       "      <th>SQFT</th>\n",
       "      <th>Home_Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Link</th>\n",
       "      <th>Zestimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4091 Midrose Trl, Dallas, TX 75287</td>\n",
       "      <td>3</td>\n",
       "      <td>1476</td>\n",
       "      <td>House</td>\n",
       "      <td>359900</td>\n",
       "      <td>https://www.zillow.com/homedetails/9028-Enchan...</td>\n",
       "      <td>508084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6205 Texana Way, Plano, TX 75074</td>\n",
       "      <td>3</td>\n",
       "      <td>1423</td>\n",
       "      <td>Townhouse</td>\n",
       "      <td>323200</td>\n",
       "      <td>https://www.zillow.com/homedetails/4091-Midros...</td>\n",
       "      <td>359903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1719 River Oaks Dr, Allen, TX 75002</td>\n",
       "      <td>3</td>\n",
       "      <td>1730</td>\n",
       "      <td>House</td>\n",
       "      <td>399900</td>\n",
       "      <td>https://www.zillow.com/homedetails/4705-Newbri...</td>\n",
       "      <td>450536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1808 Melton Dr, Carrollton, TX 75010</td>\n",
       "      <td>3</td>\n",
       "      <td>1630</td>\n",
       "      <td>House</td>\n",
       "      <td>393900</td>\n",
       "      <td>https://www.zillow.com/homedetails/8370-Hidden...</td>\n",
       "      <td>481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4908 Justin Dr, Plano, TX 75024</td>\n",
       "      <td>4</td>\n",
       "      <td>2081</td>\n",
       "      <td>House</td>\n",
       "      <td>446100</td>\n",
       "      <td>https://www.zillow.com/homedetails/1808-Melton...</td>\n",
       "      <td>404450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8370 Hidden Spring Dr, Frisco, TX 75034</td>\n",
       "      <td>4</td>\n",
       "      <td>2086</td>\n",
       "      <td>House</td>\n",
       "      <td>444900</td>\n",
       "      <td>https://www.zillow.com/homedetails/1719-River-...</td>\n",
       "      <td>419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9028 Enchanted Ridge Dr, Plano, TX 75025</td>\n",
       "      <td>4</td>\n",
       "      <td>2283</td>\n",
       "      <td>House</td>\n",
       "      <td>470900</td>\n",
       "      <td>https://www.zillow.com/homedetails/6205-Texana...</td>\n",
       "      <td>334668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4705 Newbridge Dr, McKinney, TX 75070</td>\n",
       "      <td>3</td>\n",
       "      <td>1945</td>\n",
       "      <td>House</td>\n",
       "      <td>419900</td>\n",
       "      <td>https://www.zillow.com/homedetails/1207-S-Coll...</td>\n",
       "      <td>325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1207 S College St, McKinney, TX 75069</td>\n",
       "      <td>3</td>\n",
       "      <td>1168</td>\n",
       "      <td>House</td>\n",
       "      <td>315900</td>\n",
       "      <td>https://www.zillow.com/homedetails/4908-Justin...</td>\n",
       "      <td>465800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Address  Beds  SQFT  Home_Type   Price  \\\n",
       "0        4091 Midrose Trl, Dallas, TX 75287     3  1476      House  359900   \n",
       "1          6205 Texana Way, Plano, TX 75074     3  1423  Townhouse  323200   \n",
       "2       1719 River Oaks Dr, Allen, TX 75002     3  1730      House  399900   \n",
       "3      1808 Melton Dr, Carrollton, TX 75010     3  1630      House  393900   \n",
       "4           4908 Justin Dr, Plano, TX 75024     4  2081      House  446100   \n",
       "5   8370 Hidden Spring Dr, Frisco, TX 75034     4  2086      House  444900   \n",
       "6  9028 Enchanted Ridge Dr, Plano, TX 75025     4  2283      House  470900   \n",
       "7     4705 Newbridge Dr, McKinney, TX 75070     3  1945      House  419900   \n",
       "8     1207 S College St, McKinney, TX 75069     3  1168      House  315900   \n",
       "\n",
       "                                                Link  Zestimate  \n",
       "0  https://www.zillow.com/homedetails/9028-Enchan...     508084  \n",
       "1  https://www.zillow.com/homedetails/4091-Midros...     359903  \n",
       "2  https://www.zillow.com/homedetails/4705-Newbri...     450536  \n",
       "3  https://www.zillow.com/homedetails/8370-Hidden...     481000  \n",
       "4  https://www.zillow.com/homedetails/1808-Melton...     404450  \n",
       "5  https://www.zillow.com/homedetails/1719-River-...     419500  \n",
       "6  https://www.zillow.com/homedetails/6205-Texana...     334668  \n",
       "7  https://www.zillow.com/homedetails/1207-S-Coll...     325300  \n",
       "8  https://www.zillow.com/homedetails/4908-Justin...     465800  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_scrape(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_zestimate = []\n",
    "for i in range(len(urls)):\n",
    "    link = urls[i]\n",
    "    r = s.get(link, headers=req_headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    home_value = str(soup.find_all('span', {'class':'Text-c11n-8-53-2__sc-aiai24-0 fBcVfR'})[0])\n",
    "    if not home_value:\n",
    "        home_value = soup.select_one('.zestimate').text.split()[-1]\n",
    "    else:\n",
    "        home_value = int(home_value.split('<span class=\"Text-c11n-8-53-2__sc-aiai24-0 fBcVfR\">$')[1].split('</span>')[0].replace(',',''))\n",
    "    zillow_zestimate.append(home_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>Beds</th>\n",
       "      <th>SQFT</th>\n",
       "      <th>Home_Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Link</th>\n",
       "      <th>Zestimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6205 Texana Way, Plano, TX 75074</td>\n",
       "      <td>3</td>\n",
       "      <td>1423</td>\n",
       "      <td>Townhouse</td>\n",
       "      <td>323200</td>\n",
       "      <td>https://www.zillow.com/homedetails/4600-Cedar-...</td>\n",
       "      <td>377742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4091 Midrose Trl, Dallas, TX 75287</td>\n",
       "      <td>3</td>\n",
       "      <td>1476</td>\n",
       "      <td>House</td>\n",
       "      <td>359900</td>\n",
       "      <td>https://www.zillow.com/homedetails/4091-Midros...</td>\n",
       "      <td>359903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3804 Ash Ln, McKinney, TX 75070</td>\n",
       "      <td>3</td>\n",
       "      <td>2448</td>\n",
       "      <td>House</td>\n",
       "      <td>430900</td>\n",
       "      <td>https://www.zillow.com/homedetails/12105-Chatt...</td>\n",
       "      <td>441717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1719 River Oaks Dr, Allen, TX 75002</td>\n",
       "      <td>3</td>\n",
       "      <td>1730</td>\n",
       "      <td>House</td>\n",
       "      <td>399900</td>\n",
       "      <td>https://www.zillow.com/homedetails/8370-Hidden...</td>\n",
       "      <td>481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8370 Hidden Spring Dr, Frisco, TX 75034</td>\n",
       "      <td>4</td>\n",
       "      <td>2086</td>\n",
       "      <td>House</td>\n",
       "      <td>444900</td>\n",
       "      <td>https://www.zillow.com/homedetails/1719-River-...</td>\n",
       "      <td>419500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Address  Beds  SQFT  Home_Type   Price  \\\n",
       "0         6205 Texana Way, Plano, TX 75074     3  1423  Townhouse  323200   \n",
       "1       4091 Midrose Trl, Dallas, TX 75287     3  1476      House  359900   \n",
       "2          3804 Ash Ln, McKinney, TX 75070     3  2448      House  430900   \n",
       "3      1719 River Oaks Dr, Allen, TX 75002     3  1730      House  399900   \n",
       "4  8370 Hidden Spring Dr, Frisco, TX 75034     4  2086      House  444900   \n",
       "\n",
       "                                                Link  Zestimate  \n",
       "0  https://www.zillow.com/homedetails/4600-Cedar-...     377742  \n",
       "1  https://www.zillow.com/homedetails/4091-Midros...     359903  \n",
       "2  https://www.zillow.com/homedetails/12105-Chatt...     441717  \n",
       "3  https://www.zillow.com/homedetails/8370-Hidden...     481000  \n",
       "4  https://www.zillow.com/homedetails/1719-River-...     419500  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Zestimate'] = zillow_zestimate\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Address', 'Beds', 'SQFT', 'Home_Type', 'Price', 'Link', 'Zestimate'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_zestimate = []\n",
    "for link in urls:\n",
    "    r = s.get(link, headers=req_headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    home_value = str(soup.find_all('span', {'class':'Text-c11n-8-53-2__sc-aiai24-0 fBcVfR'})[0])\n",
    "    if not home_value:\n",
    "        home_value = soup.select_one('.zestimate').text.split()[-1]\n",
    "    else:\n",
    "        home_value = int(home_value.split('<span class=\"Text-c11n-8-53-2__sc-aiai24-0 fBcVfR\">$')[1].split('</span>')[0].replace(',',''))\n",
    "    zillow_zestimate.append(home_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_zestimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = str(details[0])\n",
    "beds = int(z.split('<ul class=\"list-card-details\"><li class=\"\">')[1].split('<')[0])\n",
    "sqft = int(z.split('<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li class=\"\">')[1].split('<')[0].replace(',', ''))\n",
    "home_type = z.split('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li><li class=\"list-card-statusText\">- ')[1].split(' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beds = []\n",
    "sqft = []\n",
    "home_type = []\n",
    "\n",
    "for i in range(len(details)):\n",
    "    z = str(details[i])\n",
    "    beds.append(int(z.split('<ul class=\"list-card-details\"><li class=\"\">')[1].split('<')[0]))\n",
    "    sqft.append(int(z.split('<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li class=\"\">')[1].split('<')[0].replace(',', '')))\n",
    "    home_type.append(z.split('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li><li class=\"list-card-statusText\">- ')[1].split(' ')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = []\n",
    "#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\n",
    "for i in soup:\n",
    "    address = soup.find_all (class_= 'list-card-addr') ## no results, can get from house url?\n",
    "    price = list(soup.find_all (class_='list-card-price')) ## results, just need to re-format\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\")) ## long results, can be simplified\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'}) ## no results\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'}) ## no results\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'}) ## no results\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True)) ## no results\n",
    "    ### Stopping point\n",
    "    urls = []\n",
    "    for link in soup.find_all('a', {'class': 'list-card-link'}, href=True):\n",
    "        urls.append(link['href'])\n",
    "    urls = list(set(urls))\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df['prices'] = price\n",
    "    df['address'] = address\n",
    "    df['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df['links'] = urls\n",
    "df['links'] = df['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df['links'] = df['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df['links'] = df['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "\n",
    "for i in soup1:\n",
    "    address1 = soup1.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup1.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup1.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup1.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup1.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup1.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup1.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df1['prices'] = price1\n",
    "    df1['address'] = address1\n",
    "    df1['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup1.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df1['links'] = urls\n",
    "df1['links'] = df1['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df1['links'] = df1['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df1['links'] = df1['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "#append first two dataframes\n",
    "df = df.append(df1, ignore_index = True) \n",
    "\n",
    "#create empty dataframes\n",
    "df2 = pd.DataFrame()\n",
    "df3 = pd.DataFrame()\n",
    "df4 = pd.DataFrame()\n",
    "df5 = pd.DataFrame()\n",
    "df6 = pd.DataFrame()\n",
    "df7 = pd.DataFrame()\n",
    "df8 = pd.DataFrame()\n",
    "df9 = pd.DataFrame()\n",
    "\n",
    "for i in soup2:\n",
    "    soup = soup2\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df2['prices'] = price\n",
    "    df2['address'] = address\n",
    "    df2['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup2.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df2['links'] = urls\n",
    "df2['links'] = df2['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df2['links'] = df2['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df2['links'] = df2['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "    \n",
    "for i in soup3:\n",
    "    soup = soup3\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df3['prices'] = price1\n",
    "    df3['address'] = address1\n",
    "    df3['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup3.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df3['links'] = urls\n",
    "df3['links'] = df3['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df3['links'] = df3['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df3['links'] = df3['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup4:\n",
    "    soup = soup4\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df4['prices'] = price1\n",
    "    df4['address'] = address1\n",
    "    df4['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup4.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df4['links'] = urls\n",
    "df4['links'] = df4['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df4['links'] = df4['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df4['links'] = df4['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup5:\n",
    "    soup = soup5\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df5['prices'] = price1\n",
    "    df5['address'] = address1\n",
    "    df5['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup5.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df5['links'] = urls\n",
    "df5['links'] = df5['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df5['links'] = df5['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df5['links'] = df5['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup6:\n",
    "    soup = soup6\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df6['prices'] = price1\n",
    "    df6['address'] = address1\n",
    "    df6['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup6.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df6['links'] = urls\n",
    "df6['links'] = df6['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df6['links'] = df6['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df6['links'] = df6['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup7:\n",
    "    soup = soup7\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df7['prices'] = price1\n",
    "    df7['address'] = address1\n",
    "    df7['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup7.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df7['links'] = urls\n",
    "df7['links'] = df7['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df7['links'] = df7['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df7['links'] = df7['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup8:\n",
    "    soup = soup8\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df8['prices'] = price1\n",
    "    df8['address'] = address1\n",
    "    df8['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup8.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df8['links'] = urls\n",
    "df8['links'] = df8['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df8['links'] = df8['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df8['links'] = df8['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup9:\n",
    "    soup = soup9\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df9['prices'] = price1\n",
    "    df9['address'] = address1\n",
    "    df9['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup9.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df9['links'] = urls\n",
    "df9['links'] = df9['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df9['links'] = df9['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df9['links'] = df9['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "df = df.append(df2, ignore_index = True) \n",
    "df = df.append(df3, ignore_index = True) \n",
    "df = df.append(df4, ignore_index = True) \n",
    "df = df.append(df5, ignore_index = True) \n",
    "df = df.append(df6, ignore_index = True) \n",
    "df = df.append(df7, ignore_index = True) \n",
    "df = df.append(df8, ignore_index = True) \n",
    "df = df.append(df9, ignore_index = True) \n",
    "\n",
    "#convert columns to str\n",
    "df['prices'] = df['prices'].astype('str')\n",
    "df['address'] = df['address'].astype('str')\n",
    "df['beds'] = df['beds'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df['prices'] = df['prices'].replace('<div class=\"list-card-price\">', ' ', regex=True)\n",
    "df['address'] = df['address'].replace('<address class=\"list-card-addr\">', ' ', regex=True)\n",
    "df['prices'] = df['prices'].replace('</div>', ' ', regex=True)\n",
    "df['address'] = df['address'].replace('</address>', ' ', regex=True)\n",
    "df['prices'] = df['prices'].str.replace(r'\\D', '')\n",
    "\n",
    "#remove html tags from beds column\n",
    "df['beds'] = df['beds'].replace('<ul class=\"list-card-details\"><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bd</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li></ul>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('Studio</li><li>', '0 ', regex=True)\n",
    "\n",
    "#split beds column into beds, bath and sq_feet\n",
    "df[['beds','baths','sq_feet']] = df.beds.str.split(expand=True)\n",
    "\n",
    "#remove commas from sq_feet and convert to float\n",
    "df.replace(',','', regex=True, inplace=True)\n",
    "\n",
    "#drop nulls\n",
    "df = df[(df['prices'] != '') & (df['prices']!= ' ')]\n",
    "\n",
    "#convert column to float\n",
    "df['prices'] = df['prices'].astype('float')\n",
    "# d['sq_feet'] = df['sq_feet'].astype('float')\n",
    "\n",
    "#remove spaces from link column\n",
    "df['links'] = df.links.str.replace(' ','')\n",
    "\n",
    "print('The column datatypes are:')\n",
    "print(df.dtypes)\n",
    "print('The dataframe shape is:', df.shape)\n",
    "\n",
    "#rearrange the columns\n",
    "df = df[['prices', 'address', 'links', 'beds', 'baths', 'sq_feet']]\n",
    "\n",
    "# df\n",
    "\n",
    "#calculate the zestimate and insert into a dataframe\n",
    "zillow_zestimate = []\n",
    "for link in df['links']:\n",
    "    r = s.get(link, headers=req_headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    home_value = soup.select_one('h4:contains(\"Home value\")')\n",
    "    if not home_value:\n",
    "        home_value = soup.select_one('.zestimate').text.split()[-1]\n",
    "    else:\n",
    "        home_value = home_value.find_next('p').get_text(strip=True)\n",
    "    zillow_zestimate.append(home_value)\n",
    "\n",
    "cols=['zestimate']\n",
    "zestimate_result = pd.DataFrame(zillow_zestimate, columns=cols)\n",
    "# zestimate_result\n",
    "\n",
    "#convert zestimate column to float, and remove , and $\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace('$','')\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace('/mo','')\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace(',','')\n",
    "\n",
    "#covert rows with non zestimate to 0\n",
    "def non_zestimate(zestimate_result):\n",
    "    if len(zestimate_result['zestimate']) > 20:\n",
    "        return '0'\n",
    "    elif len(zestimate_result['zestimate']) < 5:\n",
    "        return '0'\n",
    "    else:\n",
    "        return zestimate_result['zestimate']\n",
    "\n",
    "zestimate_result['zestimate'] = zestimate_result.apply(non_zestimate,axis=1)\n",
    "\n",
    "# zestimate_result\n",
    "\n",
    "#concat zestimate dataframe and original df\n",
    "df = pd.concat([df, zestimate_result], axis=1)\n",
    "df['zestimate'] = df['zestimate'].astype('float')\n",
    "\n",
    "#create best deal column and sort by best_deal\n",
    "df ['best_deal'] = df['prices'] - df['zestimate']\n",
    "df = df.sort_values(by='best_deal')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
